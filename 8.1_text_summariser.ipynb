{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Emzee88/ISYS5002_2024_S1/blob/main/8.1_text_summariser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a summariser\n",
        "\n",
        "This section is based on the YouTube video [AI Text Summarization with Hugging Face Transformers in 4 Lines of Python](https://youtu.be/TsfLm5iiYb4)\n",
        "\n",
        "As Information Systems professionals, we use our skills to be aware of advanced concepts and think about how you can meet the organisational Using *Hugging Face Transformers*, you can leverage a pre-trained summarisation pipeline to start summarising content. In this section, we will:\n",
        "1. Installing Hugging Face Transformers\n",
        "2. Building a summarisation pipeline\n",
        "3. Run model/pipeline to summarisation\n",
        "4. **Investigate way to reuse the pipeline**\n",
        "\n",
        "> [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) free state-of-the-art pre-trained machine learning models for processing text, images, audio and video. See the project website for more information."
      ],
      "metadata": {
        "id": "WVGiU2heTdju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Hugging Face Transformers and Dependencies\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "6VaWKTfymjoQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a620f58-ce69-4297-e194-d643cb0baa33"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "from transformers import pipeline\n",
        "\n",
        "'''\n",
        "import the pipeline function from the transformers library,\n",
        "and use it to create a summarization pipeline object\n",
        "'''\n",
        "# load sumarisation pipeline\n",
        "summary_pipeline = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w5by8iL6n3Ze"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Once the pipeline is created, it can be used to summarize text\n",
        "by passing in a string of text to the summary_pipeline object\n",
        "'''\n",
        "# Let us copy-n-paste some text\n",
        "article = \"\"\"\n",
        "Around the world, as regulators look to rein in Big Tech, like the ongoing digital platforms inquiry in Australia, online platforms will face a raft of new rules in the EU.\n",
        "Known as the Digital Services Act, itâ€™s a comprehensive set of regulations for digital services and content in the Eurozone.\n",
        "Like GDPR, the Digital Services Act is expected to lead the way for other countries to provide some rules around how digital services function,\n",
        "with everything from algorithms to online marketplaces, social networks, content-sharing platforms, app stores and online travel and accommodation platforms included.\n",
        "The Digital Services Act sets out clear due diligence obligations for digital platforms and other online intermediaries with measures for cooperation with trusted flaggers and\n",
        "competent authorities on content moderation, and measures to deter rogue traders from reaching consumers.\n",
        "\"\"\"\n",
        "\n",
        "# Run the summariser pipeline\n",
        "summary = summary_pipeline(article, max_length = 50, min_length= 20)\n",
        "\n",
        "# What does a summary look like?\n",
        "print(\"summary is: \", summary)\n",
        "\n",
        "# By inspection of output, 'summary' is a list.  The first element of the list is a dictionary.\n",
        "# The key to the dictionary is 'summary_text'.\n",
        "\n",
        "# Extract and display the summarised text\n",
        "text = summary[0]['summary_text'] # get first element, then extract the value for key 'summary text\n",
        "print(\"\\nExtracted text: \", text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-ABz2tQB-nD",
        "outputId": "90b46925-de87-4e6b-b321-1cca73b48645"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "summary is:  [{'summary_text': 'The Digital Services Act is a comprehensive set of regulations for digital services and content in the Eurozone. It is expected to lead the way for other countries to provide some rules around how digital services function.'}]\n",
            "\n",
            "Extracted text:  The Digital Services Act is a comprehensive set of regulations for digital services and content in the Eurozone. It is expected to lead the way for other countries to provide some rules around how digital services function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# splits the summarised text into a list of sentences using .split('.')\n",
        "summary[0]['summary_text'].split('.')"
      ],
      "metadata": {
        "id": "qYr5o3C0dBF4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cac7fe2-17c3-4747-a06b-39e350be7ed7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The Digital Services Act is a comprehensive set of regulations for digital services and content in the Eurozone',\n",
              " ' It is expected to lead the way for other countries to provide some rules around how digital services function',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = summary[0]['summary_text'] # get first element, then extract the value for key 'summary_text'\n",
        "sentences = text.split('. ') # split the text into sentences\n",
        "\n",
        "print(\"\\nExtracted sentences: \")\n",
        "for i, sentence in enumerate(sentences):\n",
        "  print(f\"Sentence {i+1}: {sentence}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BoyMGODCr1k",
        "outputId": "2dec23fb-5155-4330-ad11-7ba7cbe10ead"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracted sentences: \n",
            "Sentence 1: The Digital Services Act is a comprehensive set of regulations for digital services and content in the Eurozone\n",
            "Sentence 2: It is expected to lead the way for other countries to provide some rules around how digital services function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's make it a function**"
      ],
      "metadata": {
        "id": "W63JcphDHKvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def summarise(article):\n",
        "  summary_pipeline = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "  summary = summary_pipeline(article, max_length = 50, min_length= 20)\n",
        "  text = summary[0]['summary_text'] # get first element, then extract the value for key 'summary text\n",
        "  return text\n"
      ],
      "metadata": {
        "id": "LQajbB93giuF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A quick test.**"
      ],
      "metadata": {
        "id": "6pBTSJ6Wg731"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "some_text = '''\n",
        "A lack of transparency and reporting standards in the scientifc community has led to increasing and widespread\n",
        "concerns relating to reproduction\n",
        "'''\n",
        "\n",
        "print(summarise(some_text))"
      ],
      "metadata": {
        "id": "qB3Tck54g-5E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e55a5da-1ce1-457b-e5d1-576970087522"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 50, but your input_length is only 31. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A lack of transparency and reporting standards in the scientifc community has led to increasing and widespread concerns relating to reproduction.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Umm... it worked, but with a warning on max_length.   We could reduce the max length or add a check that we have at least 50 words.  Our reasoning (design decision) is that it doesn't really make sense to sumarise say one sentance. We could pick any minimun size, but 50 seems like a good number.\n",
        "\n",
        "But first, how do I count words in a string?  We could search the internet for some code snippets.  We can use the the string method `split()`."
      ],
      "metadata": {
        "id": "mrEgaztkh48F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(str.split)"
      ],
      "metadata": {
        "id": "7pyqFWiQjIpO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8707014-ea7e-4bf9-a9bb-d994566215d3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on method_descriptor:\n",
            "\n",
            "split(self, /, sep=None, maxsplit=-1)\n",
            "    Return a list of the substrings in the string, using sep as the separator string.\n",
            "    \n",
            "      sep\n",
            "        The separator used to split the string.\n",
            "    \n",
            "        When set to None (the default value), will split on any whitespace\n",
            "        character (including \\\\n \\\\r \\\\t \\\\f and spaces) and will discard\n",
            "        empty strings from the result.\n",
            "      maxsplit\n",
            "        Maximum number of splits (starting from the left).\n",
            "        -1 (the default value) means no limit.\n",
            "    \n",
            "    Note, str.split() is mainly useful for data that has been intentionally\n",
            "    delimited.  With natural text that includes punctuation, consider using\n",
            "    the regular expression module.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So `split()` returns a list of words.  The `len()` of the list will be the word count.  Let us try it.\n"
      ],
      "metadata": {
        "id": "YbVR_fCEjWSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "some_text = '''\n",
        "A lack of transparency and reporting standards in the scientifc community has led to increasing and widespread\n",
        "concerns relating to reproduction\n",
        "'''\n",
        "\n",
        "count = len(some_text.split())\n",
        "print(count)"
      ],
      "metadata": {
        "id": "T3BcaeAkjyVJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ac264c6-f9eb-4c69-c5a8-7116b2c7b3aa"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let us update the function to include this (word length) check.**\n",
        "\n",
        "We will also add a doc string.  I choosen to use an `assert` statement, but you could do something similar with an `if` statement."
      ],
      "metadata": {
        "id": "gyPYdf_MkryF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def summarise(article):\n",
        "  '''\n",
        "  Returns a summary of a text.\n",
        "  The length of the text has to be greater than 50 words\n",
        "  '''\n",
        "  assert len(article.split()) > 50, 'Please make sure your text has at least 50 words'\n",
        "\n",
        "  summary_pipeline = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "  summary = summary_pipeline(article, max_length = 50, min_length= 20)\n",
        "  text = summary[0]['summary_text'] # get first element, then extract the value for key 'summary text\n",
        "  return text"
      ],
      "metadata": {
        "id": "ac9uWyG-ksjd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "some_text = '''A lack of transparency and reporting standards in the scientifc\n",
        "community has led to increasing and widespread concerns relating to reproduction\n",
        "'''\n",
        "\n",
        "print(summarise(some_text))"
      ],
      "metadata": {
        "id": "sxHTLUMWltqS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "9236e26f-ce1d-4a92-c6e0-22f901079836"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Please make sure your text has at least 50 words",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-57034c08b6e7>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m '''\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummarise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msome_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-f2c5bb611990>\u001b[0m in \u001b[0;36msummarise\u001b[0;34m(article)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mThe\u001b[0m \u001b[0mlength\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtext\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mgreater\u001b[0m \u001b[0mthan\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   '''\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Please make sure your text has at least 50 words'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0msummary_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"summarization\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"facebook/bart-large-cnn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Please make sure your text has at least 50 words"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great the assertion worked."
      ],
      "metadata": {
        "id": "VKI-tSypl_Vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigger_text='''\n",
        "A lack of transparency and reporting standards in the scientifc community has led to increasing and widespread\n",
        "concerns relating to reproduction and integrity of results. As an omics science, which generates vast amounts of data and\n",
        "relies heavily on data science for deriving biological meaning, metabolomics is highly vulnerable to irreproducibility. The\n",
        "metabolomics community has made substantial eforts to align with FAIR data standards by promoting open data formats,\n",
        "data repositories, online spectral libraries, and metabolite databases.\n",
        "'''\n",
        "\n",
        "print(summarise(bigger_text))"
      ],
      "metadata": {
        "id": "Koub-Hmel-68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a96c8fe4-f6dc-45ea-a10e-d56e8e7952fb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metabolomics generates vast amounts of data andrelies heavily on data science for deriving biological meaning. Metabolomics community has made substantial eforts to align with FAIR data standards by promoting open data formats.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay that is working well.\n",
        "\n",
        "Let us start to use our hard work\n",
        "\n",
        "*   Summarise a PDF text\n",
        "*   Summarise a webpage text\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c8wd_jI48SHf"
      }
    }
  ]
}